---
layout: post
title: Some Works abount RL in ICML 2019
---

作者·单位·问题(Motivation)·方法·结果
- 什么人，哪个单位，用了什么方法，解决了什么问题(动机是什么)，结果怎样

---

### Sergey Levine & Chelsea Finn 课题组

**Online Meta-Learning** [[Website]](https://icml.cc/Conferences/2019/ScheduleMultitrack?event=4514)
- Chelsea Finn · Aravind Rajeswaran · Sham Kakade · Sergey Levine
- UC Berkeley
- Notes
	- This work introduces an online meta-learning problem setting, which merges ideas from both the aforementioned paradigms in order to better capture the spirit and practice of continual lifelong learning.
	- FTML: practical instantiation of our approach, extending MAML meta-train on all data so far, fine-tune on current task


**Efficient Off-Policy Meta-Reinforcement Learning via Probabilistic Context Variables** [[Website]](https://icml.cc/Conferences/2019/ScheduleMultitrack?event=4607)
- Kate Rakelly · Aurick Zhou · Chelsea Finn · Sergey Levine · Deirdre Quillen
- UC Berkeley
- Algorithm: PEARL
- Github: [github.com/katerakelly/oyster](https://github.com/katerakelly/oyster)
- Notes
	- Problem: Current meta-RL methods rely heavily on on-policy experience, limiting their sample efficiency, and lack mechanisms to reason about `task uncertainty` when identifying and learning new tasks, limiting their effectiveness in sparse reward problems.
	- In this paper, we aim to address these challenges by developing an off-policy meta-RL algorithm based on `online latent task inference`.
	- Our method can be interpreted as an implementation of online probabilistic filtering of latent task variables to infer how to solve a new task from small amounts of experience.
	- Method features
		- Disentangle task inference from control
		- Off-Policy Meta-Training
		- Efficient exploration by posterior sampling
		- First off-policy meta-RL algorithm
		- 20-100X improved sample efficiency on the domains tested, often substantially better final returns
		- Probabilistic belief over the task enables posterior sampling for efficient exploration


**SOLAR: Deep Structured Representations for Model-Based Reinforcement Learning** [[Website]](https://icml.cc/Conferences/2019/ScheduleMultitrack?event=4601)
- Marvin Zhang · Sharad Vikram · Laura Smith · Pieter Abbeel · Matthew Johnson · Sergey Levine
- UC Berkeley
- Notes
	- 




PEARL: a new highly efficient meta-RL algorithm

Learning a Prior over Intent via Meta-Inverse Reinforcement Learning


---

**Making Deep Q-learning methods robust to time discretization** [[Website]](https://icml.cc/Conferences/2019/ScheduleMultitrack?event=4577)
- Corentin Tallec · Leonard Blier · Yann Ollivier
- Universit´e Paris-Sud, Facebook AI Research
- Notes
	- Problem: Deep Reinforcement Learning is not robust to hyperparameterization, implementation details, or small environment changes. 
	- In this paper, we identify sensitivity to `time discretization` in `near continuous-time environments` as a critical factor; this covers, e.g., `changing the number of frames per second`, or `the action frequency of the controller`.

**Nonlinear Distributional Gradient Temporal-Difference Learning** [[Website]](https://icml.cc/Conferences/2019/ScheduleMultitrack?event=4578)
- chao qu · Shie Mannor · Huan Xu
- Ant Financial Services Group
- Notes
	- We devise a `distributional` variant of gradient temporal-difference (TD) learning.
	- 分布式强化学习








