---
layout: post
title: Some Works abount RL and Meta-Learning in ICML 2019
---

### UC Berkeley

> Sergey Levine & Chelsea Finn Research Group

**Online Meta-Learning** [[Website]](https://icml.cc/Conferences/2019/ScheduleMultitrack?event=4514)
- Chelsea Finn · Aravind Rajeswaran · Sham Kakade · Sergey Levine
- UC Berkeley
- Notes
	- This work introduces an online meta-learning problem setting, which merges ideas from both the aforementioned paradigms in order to better capture the spirit and practice of continual lifelong learning.
	- FTML: practical instantiation of our approach, extending MAML meta-train on all data so far, fine-tune on current task


**Efficient Off-Policy Meta-Reinforcement Learning via Probabilistic Context Variables** [[Website]](https://icml.cc/Conferences/2019/ScheduleMultitrack?event=4607)
- Kate Rakelly · Aurick Zhou · Chelsea Finn · Sergey Levine · Deirdre Quillen
- UC Berkeley
- Algorithm: PEARL
- Github: [github.com/katerakelly/oyster](https://github.com/katerakelly/oyster)
- Notes
	- Problem: Current meta-RL methods rely heavily on on-policy experience, limiting their sample efficiency, and lack mechanisms to reason about `task uncertainty` when identifying and learning new tasks, limiting their effectiveness in sparse reward problems.
	- In this paper, we aim to address these challenges by developing an off-policy meta-RL algorithm based on `online latent task inference`.
	- Our method can be interpreted as an implementation of online probabilistic filtering of latent task variables to infer how to solve a new task from small amounts of experience.
	- Method features
		- Disentangle task inference from control
		- Off-Policy Meta-Training
		- Efficient exploration by posterior sampling
		- First off-policy meta-RL algorithm
		- 20-100X improved sample efficiency on the domains tested, often substantially better final returns
		- Probabilistic belief over the task enables posterior sampling for efficient exploration


**SOLAR: Deep Structured Representations for Model-Based Reinforcement Learning** [[Website]](https://icml.cc/Conferences/2019/ScheduleMultitrack?event=4601)
- Marvin Zhang · Sharad Vikram · Laura Smith · Pieter Abbeel · Matthew Johnson · Sergey Levine
- UC Berkeley, UC San Diego, Google
- Github: [github.com/sharadmv/parasol](https://github.com/sharadmv/parasol)
- Blog post: [https://bair.berkeley.edu/blog/2019/05/20/solar/](https://bair.berkeley.edu/blog/2019/05/20/solar/)
- Notes
	- Problem: Model-based reinforcement learning (RL) has proven to be a data efficient approach for learning control tasks but is difficult to utilize in domains with complex observations such as images.
	- In this paper, we present a method for learning representations that are suitable for iterative model-based policy improvement, in that these representations are optimized for inferring simple dynamics and cost models given the data from the current policy. 
	- In this work, we enable `LQR-FLM` for images using `structured representation learning`.
	- `Key idea:` structured representation learning to enable accurate modeling with simple models; model-based method that does not use forward prediction


**Learning a Prior over Intent via Meta-Inverse Reinforcement Learning** [[Website]](https://icml.cc/Conferences/2019/ScheduleMultitrack?event=5162)
- Kelvin Xu · Ellis Ratner · EECS Anca Dragan · Sergey Levine · Chelsea Finn
- UC Berkeley
- Notes
	- Problem: In practice, IRL must commonly be performed with only a limited set of demonstrations where it can be exceedingly difficult to unambiguously recover a reward function.
	- In this work, we exploit the insight that demonstrations from other tasks can be used to constrain the set of possible reward functions by learning a ''prior'' that is specifically optimized for the ability to infer expressive reward functions from limited numbers of demonstrations.
	- Motivation: a well specified reward function remains an important assumption for applying RL in practice
		- Often easier to provide expert data and learn a reward function using inverse RL
		- Inverse RL frequently requires a lot of data to learn a generalizable reward
			- This is due in part with the fundamental ambiguity of reward learning
	- Goal: how can agents infer rewards from one or a few demonstrations?
		- Intuition: demonstrations from previous tasks induce a prior over the space of possible future tasks
	- Meta-inverse reinforcement learning: using prior tasks information to accelerate inverse-RL
	- Our approach: Meta reward and intention learning
	- Experiments
		- Domain 1: SpriteWorld environment
		- Domain 2: First person navigation (SUNCG)

### DeepMind

**Composing Entropic Policies using Divergence Correction** [[Website]](https://icml.cc/Conferences/2019/ScheduleMultitrack?event=4579)
- Jonathan Hunt · Andre Barreto · Timothy Lillicrap · Nicolas Heess
- DeepMind
- Notes
	- Problem: Deep reinforcement learning algorithms have achieved remarkable successes, but often require vast amounts of experience to solve a task. `Composing skills mastered in one task` in order to efficiently solve novel challenges promises dramatic improvements in data efficiency.
	- Here, we build on two recent works composing behaviors represented in the form of action-value functions.
	- 组合不同策略


**Learning Latent Dynamics for Planning from Pixels** [[Website]](https://icml.cc/Conferences/2019/ScheduleMultitrack?event=5147)
- Danijar Hafner · Timothy Lillicrap · Ian Fischer · Ruben Villegas · David Ha · Honglak Lee · James Davidson
- DeepMind
- Algorithm: PlaNet
- Github: [github.com/google-research/planet](https://github.com/google-research/planet)
- Blog post: [https://ai.googleblog.com/2019/02/introducing-planet-deep-planning.html](https://ai.googleblog.com/2019/02/introducing-planet-deep-planning.html)
- Notes
	- Problem: Learning dynamics models that are accurate enough for planning has been a long-standing challenge, especially in image-based domains.
	- We propose the Deep Planning Network (PlaNet), a purely model-based agent that learns the environment dynamics from images and chooses actions through fast online planning in latent space. 
	- Method features
		- Recipe for scalable model-based reinforcement learning
		- Efficient planning in latent space with large batch size
		- Reaches top performance using 200X fewer episodes


**Policy Consolidation for Continual Reinforcement Learning** [[Website]](https://icml.cc/Conferences/2019/ScheduleMultitrack?event=4582)
- Christos Kaplanis · Murray Shanahan · Claudia Clopath
- Imperial College London, DeepMind
- Notes
	- Problem: Catastrophic forgetting in deep reinforcement learning
	- We propose a method for tackling catastrophic forgetting in deep reinforcement learning that is agnostic to the timescale of changes in the distribution of experiences, does not require knowledge of task boundaries and can adapt in continuously changing environments. 
	- Future work
		- Prioritised consolidation
		- Adapt for off-policy learning






### Others

**Making Deep Q-learning methods robust to time discretization** [[Website]](https://icml.cc/Conferences/2019/ScheduleMultitrack?event=4577)
- Corentin Tallec · Leonard Blier · Yann Ollivier
- Universit´e Paris-Sud, Facebook AI Research
- Notes
	- Problem: Deep Reinforcement Learning is not robust to hyperparameterization, implementation details, or small environment changes. 
	- In this paper, we identify sensitivity to `time discretization` in `near continuous-time environments` as a critical factor; this covers, e.g., `changing the number of frames per second`, or `the action frequency of the controller`.


**Nonlinear Distributional Gradient Temporal-Difference Learning** [[Website]](https://icml.cc/Conferences/2019/ScheduleMultitrack?event=4578)
- chao qu · Shie Mannor · Huan Xu
- Ant Financial Services Group
- Notes
	- We devise a `distributional` variant of gradient temporal-difference (TD) learning.
	- 分布式强化学习


**TibGM: A Transferable and Information-Based Graphical Model Approach for Reinforcement Learning** [[Website]](https://icml.cc/Conferences/2019/ScheduleMultitrack?event=4580)
- Tameem Adel · Adrian Weller
- Machine Learning Group, University of Cambridge
- Notes
	- Problem: Hierarchical reinforcement learning (HRL) can provide a principled solution to the RL challenge of scalability for complex tasks. By incorporating a graphical model (GM) and the rich family of related methods, there is also hope to address issues such as `transferability, generalisation and exploration`. 
	- We propose a flexible GM-based HRL framework which leverages efficient inference procedures to enhance generalisation and transfer power. 


**Multi-Agent Adversarial Inverse Reinforcement Learning** [[Website]](https://icml.cc/Conferences/2019/ScheduleMultitrack?event=4581)
- Lantao Yu · Jiaming Song · Stefano Ermon
- Stanford University
- Notes
	- Problem: Inverse reinforcement learning provides a framework to automatically acquire suitable reward functions from expert demonstrations. Its extension to multi-agent settings, however, is difficult due to the more complex notions of rational behaviors.
	- In this paper, we propose MA-AIRL, a new framework for multi-agent inverse reinforcement learning, which is effective and scalable for Markov games with high-dimensional state-action space and unknown dynamics. 


**Off-Policy Deep Reinforcement Learning without Exploration** [[Website]](https://icml.cc/Conferences/2019/ScheduleMultitrack?event=4583)
- Scott Fujimoto · David Meger · Doina Precup
- McGill University
- Github: [github.com/sfujim/BCQ](https://github.com/sfujim/BCQ)
- Notes
	- Problem: Many practical applications of reinforcement learning constrain agents to learn from `a fixed batch of data which has already been gathered`, without offering further possibility for data collection.
	- In this paper, we demonstrate that due to errors introduced by extrapolation, standard off-policy deep reinforcement learning algorithms, such as DQN and DDPG, are incapable of learning with `data uncorrelated to the distribution under the current policy`, making them ineffective for this fixed batch setting. 
	- We introduce a novel class of off-policy algorithms, batch-constrained reinforcement learning, which restricts the action space in order to force the agent towards behaving close to on-policy with respect to a subset of the given data. 


**Random Expert Distillation: Imitation Learning via Expert Policy Support Estimation** [[Website]](https://icml.cc/Conferences/2019/ScheduleMultitrack?event=4584)
- Ruohan Wang · Carlo Ciliberto · Pierluigi Vito Amadori · Yiannis Demiris
- Imperial College London
- Github: [github.com/RuohanW/RED](https://github.com/RuohanW/RED)
- Notes:
	- Problem: 
		- Imitation learning from a finite set of expert trajectories, without access to reinforcement signals
		- The classical approach of extracting the expert's reward function via inverse reinforcement learning, followed by reinforcement learning is indirect and may be computationally expensive. 
	- `Random Expert Distillation` is a new framework for imitation learning, using the estimated support of the expert policy as reward.


**Revisiting the Softmax Bellman Operator: New Benefits and New Perspective** [[Website]](https://icml.cc/Conferences/2019/ScheduleMultitrack?event=4585)
- Zhao Song · Ron Parr · Lawrence Carin
- Baidu Research, Duke University
- Github: [github.com/zhao-song/Softmax-DQN](https://github.com/zhao-song/Softmax-DQN)
- Notes
	- Problem: The impact of `softmax on the value function` itself in reinforcement learning (RL) is often viewed as problematic because it leads to sub-optimal value (or Q) functions and interferes with the contraction properties of the `Bellman operator`.
	- 探究Q-Learning中贝尔曼方程的最大化操作带来的影响

**Distributional Reinforcement Learning for Efficient Exploration** [[Website]](https://icml.cc/Conferences/2019/ScheduleMultitrack?event=5157)
- Borislav Mavrin · Hengshuai Yao · Linglong Kong · Kaiwen Wu · Yaoliang Yu
- University of Alberta, Huawei Technologies, University of Waterloo
- Notes
	- Problem: In distributional reinforcement learning (RL), the estimated distribution of value functions model both the `parametric and intrinsic uncertainties`. 
	- We propose a novel and efficient exploration method for deep RL that has two components. 
		- The first is a decaying schedule to suppress the intrinsic uncertainty. 
		- The second is an exploration bonus calculated from the upper quantiles of the learned distribution.
	- A particularly interesting application of the (Distributional) RL
approach is driving safety.


**Learning Action Representations for Reinforcement Learning** [[Website]](https://icml.cc/Conferences/2019/ScheduleMultitrack?event=4522)
- Yash Chandak · Georgios Theocharous · James Kostas · Scott Jordan · Philip Thomas
- University of Massachusetts Amherst, `Adobe Research`
- Notes
	- Problem: Most model-free reinforcement learning methods leverage state representations (embeddings) for generalization, but either ignore structure in the space of actions or assume the structure is provided a priori.
	- We show how a policy can be decomposed into a component that acts in a low-dimensional space of action representations and a component that transforms these representations into actual actions. 
	- Key Insights
		- Actions are not independent discrete quantities.
		- There is a low dimensional structure underlying their behavior pattern.
		- This structure can be learned independent of the reward.
		- Instead of raw actions, agent can act in this space of behavior and feedback can be generalized to similar actions.
	- Algorithm
		- (a) Supervised learning of action representations.
		- (b) Learning internal policy with policy gradients.


**Control Regularization for Reduced Variance Reinforcement Learning** [[Website]](https://icml.cc/Conferences/2019/ScheduleMultitrack?event=4610)
- Richard Cheng · Abhinav Verma · Gabor Orosz · Swarat Chaudhuri · Yisong Yue · Joel Burdick
- California Institute of Technology, Rice University, University of Michigan, Caltech
- Github: [github.com/rcheng805/CORE-RL](https://github.com/rcheng805/CORE-RL)
- Notes
	- Problem: Dealing with `high variance` is a significant challenge in model-free reinforcement learning (RL). Existing methods are unreliable, exhibiting high variance in performance from run to run using different initializations/seeds. 
	- Focusing on problems arising in continuous control, we propose a functional regularization approach to augmenting model-free RL.
	- Control Regularization helps by providing:
		- Reduced variance
		- Higher rewards
		- Faster learning
		- Potential safety guarantees


**On the Generalization Gap in Reparameterizable Reinforcement Learning** [[Website]](https://icml.cc/Conferences/2019/ScheduleMultitrack?event=4611)
- Huan Wang · Stephan Zheng · Caiming Xiong · Richard Socher
- Salesforce Research
- Notes
	- Problem: Understanding generalization in reinforcement learning (RL) is a significant challenge, as many common assumptions of traditional supervised learning theory do not apply. 
	- We argue that the gap between training and testing performance of RL agents is caused by two types of errors: intrinsic error due to the randomness of the environment and an agent's policy, and external error by the change of environment distribution.






> 作者·单位·问题(Motivation)·方法·结果
> - 什么人，哪个单位，用了什么方法，解决了什么问题(动机是什么)，结果怎样







