---
layout: post
title: 探秘：网易雷火伏羲实验室
tags: [AI Lab, reinforcement learning, job]
---

实验室主页：[http://fuxi.163.com/](http://fuxi.163.com/)

实验室介绍：

- 网易伏羲实验室是国内首家专业游戏AI研究机构，美国工程院外籍院士，全球人工智能领域杰出专家 - 沈向洋博士（Dr. Harry Shum）受邀担任伏羲实验室首席顾问，实验室以“创建人工智能与游戏交叉领域的世界一流实验室”为宗旨，运用人工智能尖端技术为玩家营造新世代的游戏体验，同时借助游戏平台的海量数据和仿真环境，推动人工智能技术的发展。

- 实验室的研究方向涵盖了大数据平台，用户画像，强化学习，图像动作，自然语言处理，语音合成及音乐生成等众多领域，力图全方位发掘人工智能技术在游戏中的应用潜力。我们的团队汇集了来自清华大学，北京大学，浙江大学，中国科学技术大学，武汉大学等著名学府的众多尖端人才，既有深厚的学术背景，亦有强大的工程能力。

相关报道：

- NeurIPS 2018网易推出强化编程框架 [[Blog]](https://mp.weixin.qq.com/s/HzQIfyY5nvTP6Cn48hEaBA)
- 网易伏羲AI实验室负责人李仁杰：人工智能在游戏中的赋能与落地(2019-02-28) [[Slide]](/topics/data/2019-人工智能在游戏中的赋能与落地.pdf)
- 人工智能改变游戏未来？网易伏羲AI Lab展示游戏开发黑科技(2019-05-22) [[机器之心]](https://www.jiqizhixin.com/articles/2019-05-22-14)

[强化学习研究的方向](http://fuxi.163.com/tech.html#qhxx)：

> 目标：通过人机和自我对战，自动探索游戏各职业的最佳战斗策略，培养进攻、防守、骚扰等战术风格，乃至模仿玩家的操作，让众多个性分明的AI伴随玩家共同成长，共同进步。

- `可迁移的强化学习算法`
	- 研究可迁移的强化学习算法， 提高模型在不同游戏环境中的复用性和训练效率。
- `分布式强化学习算法`
	- 研究现有强化学习算法在大规模分布式训练平台上的可行性和收敛性。
- `（自动）分层强化学习`
	- 对于极度复杂的问题，通常需要对任务做（自动）分解，让强化学习能够顺利解决。
- `多智能体强化学习`
	- 在游戏场景中，多个智能体之间存在合作同时也存在竞争， 利用强化学习能够训练出具有群体智能的AI。
- `难度风格可控AI的生成`
	- 根据实际项目的需要，能够自动生成难度不同、风格迥异的AI。
- `经验池管理`
	- 优化强化学习中经验池管理策略，提高强化学习算法的样本利用率并减少网络的遗忘特性。
- `基于树模型的强化学习算法`
	- 权衡神经网络模型和树模型的优缺点，开发出更高效、易扩展和易解释的强化学习算法。
- `模仿学习`
	- 模仿学习希望AI能够表现更像玩家，模仿学习需要解决的问题有：
		1. 多种操作风格专家样本数据下的模仿学习。
		2. 在博弈等高不确定性环境下的模仿学习。
		3. 研究普适的专家样本和环境reward相结合的模仿学习框架。
- `进化强化学习`
	- 结合进化算法和强化学习的优势以提高AI优化效率。一方面，利用进化算法跳出局部最优并自动优化模型超参；另一方面，利用强化学习提高开采效率并抵抗环境噪声。
- `AutoML`
	- 研究自动训练机器学习模型的技术。如神经网络结构设计、超参自动调整等。

强化学习典型案例：

- 《潮人篮球》对战机器人
	- 多智能体系统是多个具有独立观察、推理、决策能力的智能体在特定环境中进行交互所构成系统，是对现实生活中复杂的场景的建模。多智能体强化学习是利用强化学习来解决多智能体问题，区别于单智能的强化学习，多智能体强化学习面临着由更大的环境不确定性、学习不稳定性、信度分配、通信限制等问题带来的更多挑战。我们在潮人篮球中成功运用了多智能体强化学习，训练出了3v3的强化学习机器人，获得产品的认可，并正式上线运行。

<p style="text-align:center">
	<img src="/topics/img/fuxi-chaoren.jpg" />
</p>


强化学习相关论文发表：

- NIPS-2018: A Deep Bayesian Policy Reuse Approach Against Non-Stationary Agents (在线对手建模)
- IJCAI-2018: Recurrent Deep Multiagent Q-Learning for Autonomous Brokers in Smart Grid [[Paper]](https://www.ijcai.org/proceedings/2018/0079.pdf)
- IJCAI-2019: Reinforcement Learning Experience Reuse with Policy Residual Representation (策略迁移) [[Paper]](https://arxiv.org/pdf/1905.13719v1.pdf)
- IJCAI-2019: Value Function Transfer for Deep Multi-Agent Reinforcement Learning Based on N-Step Returns (Multi-Agent环境下的稀疏交互)
- IJCAI-2019: Deep Multi-Agent Reinforcement Learning with Discrete-Continuous Hybrid Action Spaces (混合动作空间) [[Paper]](https://arxiv.org/pdf/1903.04959)[[机器之心报道]](https://www.jiqizhixin.com/articles/2019-06-06-9?from=synced&keyword=%E4%BC%8F%E7%BE%B2)
- IJCAI-2019: Explicitly Coordinated Policy Iteration