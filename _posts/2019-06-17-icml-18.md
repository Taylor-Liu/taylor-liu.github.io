---
layout: post
title: Some Works abount RL in ICML 2018
tags: [ICML, reinforcement learning]
---

**Automatic Goal Generation for Reinforcement Learning Agents**
- [Carlos Florensa](https://sites.google.com/view/carlosflorensa) · David Held · Xinyang Geng · Pieter Abbeel
- UC Berkeley, CMU, OpenAI
- [Website](https://sites.google.com/view/goalgeneration4rl) [[Paper]](http://proceedings.mlr.press/v80/florensa18a.html) [[Github]](https://github.com/florensacc/rllab-curriculum) [[Talk]](https://vimeo.com/312269573) 
- Abstract
	- Reinforcement learning (RL) is a powerful technique to train an agent to perform a task; however, an agent that is trained using RL is only capable of achieving the single task that is specified via its reward function.   Such an approach does not scale well to settings in which an agent needs to perform a diverse set of tasks, such as navigating to varying positions in a room or moving objects to varying locations.  Instead, we propose a method that allows an agent to automatically discover the range of tasks that it is capable of performing in its environment.  We use a generator network to propose tasks for the agent to try to accomplish, each task being specified as reaching a certain parametrized subset of the state-space.  The generator network is optimized using adversarial training to produce tasks that are always at the appropriate level of difficulty for the agent, thus automatically producing a curriculum.  We show that, by using this framework, an agent can efficiently and automatically learn to perform a wide set of tasks without requiring any prior knowledge of its environment, even when only sparse rewards are available.


**Time Limits in Reinforcement Learning**
- [Fabio Pardo](https://fabiopardo.github.io/) · Arash Tavakoli · Vitaly Levdik · Petar Kormushev
- Imperial College London
- [[Paper]](http://proceedings.mlr.press/v80/pardo18a.html) [[Talk]](https://www.youtube.com/watch?v=UUqidOSMKLE)
- Abstract
	- In reinforcement learning, it is common to let an agent interact for a fixed amount of time with its environment before resetting it and repeating the process in a series of episodes. The task that the agent has to learn can either be to maximize its performance over `(i) that fixed period`, or `(ii) an indefinite period where time limits are only used during training to diversify experience`. In this paper, we provide a formal account for `how time limits could effectively be handled in each of the two cases and explain why not doing so can cause state-aliasing and invalidation of experience replay, leading to suboptimal policies and training instability`. `In case (i)`, we argue that the terminations due to time limits are in fact part of the environment, and thus a notion of the remaining time should be included as part of the agent's input to avoid violation of the Markov property. `In case (ii)`, the time limits are not part of the environment and are only used to facilitate learning. We argue that this insight should be incorporated by bootstrapping from the value of the state at the end of each partial episode. For both cases, we illustrate empirically the significance of our considerations in improving the performance and stability of existing reinforcement learning algorithms, showing state-of-the-art results on several control tasks.


**Latent Space Policies for Hierarchical Reinforcement Learning**
- [Tuomas Haarnoja](https://people.eecs.berkeley.edu/~haarnoja/) · Kristian Hartikainen · Pieter Abbeel · Sergey Levine
- UC Berkeley, OpenAI
- [[Website]](https://sites.google.com/view/latent-space-deep-rl) [[Paper]](https://arxiv.org/abs/1804.02808) [[Github]](https://github.com/haarnoja/sac/) [[Talk]](https://vimeo.com/312265271)
- Abstract
	- We address the problem of learning hierarchical deep neural network policies for reinforcement learning. In contrast to methods that explicitly restrict or cripple lower layers of a hierarchy to force them to use higher-level modulating signals, each layer in our framework is trained to directly solve the task, but acquires a range of diverse strategies via a maximum entropy reinforcement learning objective. Each layer is also augmented with latent random variables, which are sampled from a prior distribution during the training of that layer. The maximum entropy objective causes these latent variables to be incorporated into the layer's policy, and the higher level layer can directly control the behavior of the lower layer through this latent space. Furthermore, by constraining the mapping from latent variables to actions to be invertible, higher layers retain full expressivity: neither the higher layers nor the lower layers are constrained in their behavior. Our experimental evaluation demonstrates that we can improve on the performance of single-layer policies on standard benchmark tasks simply by adding additional layers, and that our method can solve more complex sparse-reward tasks by learning higher-level policies on top of high-entropy skills optimized for simple low-level objectives.


**Universal Planning Networks: Learning Generalizable Representations for Visuomotor Control**
- [Aravind Srinivas](https://people.eecs.berkeley.edu/~aravind/) · Allan Jabri · Pieter Abbeel · Sergey Levine · Chelsea Finn
- UC Berkeley, OpenAI
- [[Paper]](http://proceedings.mlr.press/v80/srinivas18b.html) [[Github]](https://github.com/aravind0706/upn) [[Talk]](https://vimeo.com/312270148)
- Abstract
	- A key challenge in complex visuomotor control is learning abstract representations that are effective for specifying goals, planning, and generalization. To this end, we introduce universal planning networks (UPN). UPNs embed differentiable planning within a goal-directed policy. This planning computation unrolls a forward model in a latent space and infers an optimal action plan through gradient descent trajectory optimization. The plan-by-gradient-descent process and its underlying representations are learned end-to-end to directly optimize a supervised imitation learning objective. We find that the representations learned are not only effective for goal-directed visual imitation via gradient-based trajectory optimization, but can also provide a metric for specifying goals using images. The learned representations can be leveraged to specify distance-based rewards to reach new target states for model-free reinforcement learning, resulting in substantially more effective learningwhen solving new tasks described via image based goals. We were able to achievesuccessful transfer of visuomotor planning strategies across robots with significantly different morphologies and actuation capabilities. Visit [https://sites.google.com/view/upn-public/home](https://sites.google.com/view/upn-public/home) for video highlights.












