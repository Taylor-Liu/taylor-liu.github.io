---
layout: post
title: From Q-Learning to DQN
tags: [reinforcement learning, dqn]
---

---
**Table of Contents**
* TOC
{:toc}
---

## Model-Free RL: Q-Learning

- Run policy: Step in env with action from $$Q_{\theta}$$, store to replay buffer
- Evaluate policy: Update $$Q_{\theta}$$ to minimize Bellman error, using all previous data (**off-policy**)
- Improve policy: $$a^{*}=\arg \max _{a} Q_{\theta}(s, a)$$

## Q-Learning Updates by Bootstrapping

- Collect experience in the environment using a policy which trades off between acting randomly and acting according to current $$Q_{\theta}$$
- Interleave data collection with updates to $$Q_{\theta}$$ to minimize Bellman error by bootstrapping:

$$
\min _{\theta} \sum_{\left(s, a, s^{\prime}, r\right) \in \mathcal{D}}\left(Q_{\theta}(s, a)-y\left(s^{\prime}, r\right)\right)^{2}
$$

where

$$
y\left(s^{\prime}, r\right)=r+\gamma \max _{a^{\prime}} Q_{\theta}\left(s^{\prime}, a^{\prime}\right),
$$

and gradients don't propagate through y.

## Getting Q-Learning to Work (DQN)

### Experience replay

- Data distribution changes over time: as your $$Q$$ function gets better and you exploit this, you visit $$(s,a,s^{'},r)$$ transitions than you did earlier
- Stabilize learning by keeping old transitions in a replay buffer, and taking minibatch gradient descent on mix of old and new transitions

### Target networks

- Bootstrapping with function approximators is unstable!
- It's like regression, but it's not: targets depend on parameters $$\theta$$ -- so an update to $$Q$$ changes the target!

$$
y\left(s^{\prime}, r\right)=r+\gamma \max _{a^{\prime}} Q_{\theta}\left(s^{\prime}, a^{\prime}\right)
$$

- Stabilize it by holding the target fixed for a while: keep a separate target network, $$Q_{\theta_{\text {targ }}}$$, and every $$k$$ steps update $$\theta_{\text {targ}} \leftarrow \theta$$
- If unstable, why do it? Because it works well! (Plus theory, later)

## In DQN, Action Space Matters

What we've described so far requires the computation of

$$
\max _{a} Q_{\theta}(s, a),
$$

both for selecting actions and calculating update targets.

**Problem**: for continuous action spaces, this is nontrivially hard! DQN therefore only applies for **discrete actions**, because we can output one Q-value per action from the network.

## DQN Pseudocode

















