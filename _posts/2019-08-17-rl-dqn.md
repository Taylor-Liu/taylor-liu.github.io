---
layout: post
title: From Q-Learning to DQN
tags: [reinforcement learning, dqn]
---

---
**Table of Contents**
* TOC
{:toc}
---

## Model-Free RL: Q-Learning

- Run policy: Step in env with action from $$Q_{\theta}$$, store to replay buffer
- Evaluate policy: Update $$Q_{\theta}$$ to minimize Bellman error, using all previous data (**off-policy**)
- Improve policy: $$a^{*}=\arg \max _{a} Q_{\theta}(s, a)$$

## Q-Learning Updates by Bootstrapping

- Collect experience in the environment using a policy which trades off between acting randomly and acting according to current $$Q_{\theta}$$
- Interleave data collection with updates to $$Q_{\theta}$$ to minimize Bellman error by bootstrapping:

$$
\min _{\theta} \sum_{\left(s, a, s^{\prime}, r\right) \in \mathcal{D}}\left(Q_{\theta}(s, a)-y\left(s^{\prime}, r\right)\right)^{2}
$$

where

$$
y\left(s^{\prime}, r\right)=r+\gamma \max _{a^{\prime}} Q_{\theta}\left(s^{\prime}, a^{\prime}\right)
$$

and gradients don't propagate through y.

## Getting Q-Learning to Work (DQN)

### Experience replay

- Data distribution changes over time: as your $$Q$$ function gets better and you exploit this, you visit $$(s,a,s^{'},r)$$ transitions than you did earlier
- Stabilize learning by keeping old transitions in a replay buffer, and taking minibatch gradient descent on mix of old and new transitions